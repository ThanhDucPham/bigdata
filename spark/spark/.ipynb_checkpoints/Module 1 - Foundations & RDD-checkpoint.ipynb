{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intrduction to Spark\n",
    "* At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. \n",
    "* The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. \n",
    "* RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. \n",
    "* Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. \n",
    "* Finally, RDDs automatically recover from node failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### SparkContext\n",
    "* Handler to Spark Cluster\n",
    "* Useful for creating RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create entry points to spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conf = SparkConf().setAppName('anyappname').setMaster('localhost')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "* A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. \n",
    "* Represents an immutable, partitioned collection of elements that can be operated on in parallel.\n",
    "\n",
    "### Two ways to create RDD\n",
    "* parallelize - from collection\n",
    "* textFile - from external file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelized Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### External Datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_names = sc.textFile('data/baby_names_beginning_2007.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year,First Name,County,Sex,Count\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile('data/baby_names_beginning_2007.csv')\n",
    "\n",
    "#Prints the first element\n",
    "print(lines.first())\n",
    "#Each element of is one line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Year,First Name,County,Sex,Count',\n",
       " u'2013,GAVIN,ST LAWRENCE,M,9',\n",
       " u'2013,LEVI,ST LAWRENCE,M,9',\n",
       " u'2013,LOGAN,NEW YORK,M,44',\n",
       " u'2013,HUDSON,NEW YORK,M,49']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns first 5 elemets\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 26, 25, 24, 25]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of first 5 elements\n",
    "lines.map(lambda s: len(s)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2424036\n"
     ]
    }
   ],
   "source": [
    "#returns total number of characters\n",
    "rdd = lines.map(lambda s: len(s))\n",
    "rdd = rdd.map(lambda s: 2*s)\n",
    "print(rdd.reduce( lambda a,b: a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key-Value Pairs RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"hello\",\"world\",\"good\",\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1), ('world', 1), ('good', 1), ('hello', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.map(lambda w: (w,1))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* value corresponding to same key undergoes lambda operation\n",
    "* Note: Any function which has (key,value) pair can be worked on by {Any}ByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 1), ('hello', 2), ('good', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation\n",
    "* Eg - map, filter, flatMap ...\n",
    "* Changes data from one format to another\n",
    "* Lazy execution - Delays execution untill finds an 'Action' so that it can prepare optimized lineage ( spark internal code pipeline )\n",
    "\n",
    "#### Actions\n",
    "* Eg - count, collect, reduce ...\n",
    "* Trigger execution of pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "* map(func)\tReturn a new distributed dataset formed by passing each element of the source through a function func.\n",
    "* filter(func)\tReturn a new dataset formed by selecting those elements of the source on which func returns true.\n",
    "* flatMap(func)\tSimilar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n",
    "* mapPartitions(func)\tSimilar to map, but runs separately on each partition (block) of the RDD.\n",
    "* mapPartitionsWithIndex(func)\tSimilar to mapPartitions, but also provides func with an integer value representing the index of the partition, \n",
    "* sample(withReplacement, fraction, seed)\tSample a fraction fraction of the data, with or without replacement, using a given random number generator seed.\n",
    "* union(otherDataset)\tReturn a new dataset that contains the union of the elements in the source dataset and the argument.\n",
    "* intersection(otherDataset)\tReturn a new RDD that contains the intersection of elements in the source dataset and the argument.\n",
    "* distinct([numTasks]))\tReturn a new dataset that contains the distinct elements of the source dataset.\n",
    "* groupByKey([numTasks])\tWhen called on a dataset of (K, V) pairs, \n",
    "\n",
    "* reduceByKey(func, [numTasks])\tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "\n",
    "* aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])\tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "* sortByKey([ascending], [numTasks])\tWhen called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.\n",
    "* join(otherDataset, [numTasks])\tWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.\n",
    "* cogroup(otherDataset, [numTasks])\tWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith.\n",
    "* cartesian(otherDataset)\tWhen called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).\n",
    "* pipe(command, [envVars])\tPipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.\n",
    "* coalesce(numPartitions)\tDecrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.\n",
    "* repartition(numPartitions)\tReshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.\n",
    "* repartitionAndSortWithinPartitions(partitioner)\tRepartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action\n",
    "\n",
    "* reduce(func)\tAggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n",
    "* collect()\tReturn all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.\n",
    "* count()\tReturn the number of elements in the dataset.\n",
    "* first()\tReturn the first element of the dataset (similar to take(1)).\n",
    "* take(n)\tReturn an array with the first n elements of the dataset.\n",
    "* takeSample(withReplacement, num, [seed])\tReturn an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.\n",
    "* takeOrdered(n, [ordering])\tReturn the first n elements of the RDD using either their natural order or a custom comparator.\n",
    "* saveAsTextFile(path)\tWrite the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.\n",
    "* saveAsSequenceFile(path) \n",
    "(Java and Scala)\tWrite the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).\n",
    "* saveAsObjectFile(path) \n",
    "(Java and Scala)\tWrite the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile().\n",
    "* countByKey()\tOnly available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.\n",
    "* foreach(func)\tRun a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems. \n",
    "Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details.\n",
    "\n",
    "\n",
    "##### Note : We will dig deeper into many of these functions in a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Operations\n",
    "* Many operations in spark trigger shuffle .i.e movement of data across one one to another.\n",
    "* Data movement is expensive & should be as less as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates 2 partition\n",
    "rdd = sc.parallelize([\"hello\",\"world\",\"good\",\"hello\"],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world'], ['good', 'hello']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#glom - returns data in one partition in list\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda w:(w,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('hello', 1), ('world', 1)], [('good', 1), ('hello', 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduceByKey - generates a new RDD where all the values of same key are tupled\n",
    "rdd = rdd.reduceByKey(lambda a,b:(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 1), ('good', 1), ('hello', (1, 1))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [('world', 1), ('good', 1), ('hello', (1, 1))]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupByKey - ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([\n",
    "    (\"USA\", 1), (\"USA\", 2), (\"India\", 1),\n",
    "    (\"UK\", 1), (\"India\", 4), (\"India\", 9),\n",
    "    (\"USA\", 8), (\"USA\", 3), (\"India\", 4),\n",
    "    (\"UK\", 6), (\"UK\", 9), (\"UK\", 5), (\"VietNam\",5)], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "y = x.groupByKey(3)\n",
    "print(y.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('USA', 1), ('USA', 2), ('India', 1), ('UK', 1)],\n",
       " [('India', 4), ('India', 9), ('USA', 8), ('USA', 3)],\n",
       " [('India', 4), ('UK', 6), ('UK', 9), ('UK', 5), ('VietNam', 5)]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [('UK', <pyspark.resultiterable.ResultIterable at 0x7fa180677cd0>)],\n",
       " [('VietNam', <pyspark.resultiterable.ResultIterable at 0x7fa180682550>),\n",
       "  ('India', <pyspark.resultiterable.ResultIterable at 0x7fa180682290>),\n",
       "  ('USA', <pyspark.resultiterable.ResultIterable at 0x7fa180682210>)]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('check>', 'UK', [1, 6, 9, 5])\n",
      "('check>', 'VietNam', [5])\n",
      "('check>', 'India', [1, 4, 9, 4])\n",
      "('check>', 'USA', [1, 2, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "for t in y.collect():\n",
    "    print('check>',t[0], list(t[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReduceByKey\n",
    "* The above operation brings all data with same key in one node\n",
    "* This operation causes data shuffling\n",
    "* much better in large dataset: combine output with common key on each partition before shuffling the data\n",
    "\n",
    "#### GroupByKey\n",
    "* Costly: doesn't use combiner local to a partition to reduce the data transfer\n",
    "* results in Hash-Partitioned RDDs\n",
    "\n",
    "#### Note - We can reduce shuffle using groupBykey ( we will see soon )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Opearations\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregate\n",
    "* Aggregate the elements of each partition.\n",
    "* Aggregate the result of each partition\n",
    "* 'zero_value' is default init value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "print(sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp))\n",
    "\n",
    "print(sc.parallelize([]).aggregate((0, 0), seqOp, combOp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregateByKey\n",
    "* seqOp works on each partition\n",
    "* combOp works on result of each partitions\n",
    "* ByKey causes operations on data with same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('hello', 1)], [('good', 2)], [('hello', 3)], [('food', 4)]]\n",
      "[('food', (4, 1)), ('hello', (4, 2)), ('good', (2, 1))]\n"
     ]
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "rdd = sc.parallelize([('hello',1), ('good',2), ('hello',3), ('food',4)])\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.aggregateByKey((0, 0), seqOp, combOp).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cache\n",
    "* Prevent re-computation of RDD\n",
    "* In-memory caching wherever computation is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "10000000\n"
     ]
    }
   ],
   "source": [
    "#Using cache() persists \n",
    "rdd = sc.parallelize(range(10000000)) #first time this line will be executed\n",
    "rdd.cache()\n",
    "rdd1 = rdd.map(lambda x: x+2)\n",
    "rdd2 = rdd.map(lambda x:x+3)\n",
    "print(rdd1.count())\n",
    "print(rdd2.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[70] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove data from cache\n",
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Opeartion\n",
    "\n",
    "* cartesian\n",
    "* union\n",
    "* intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(1,10))\n",
    "rdd2 = sc.parallelize(range(11,20))\n",
    "rdd3 = sc.parallelize(range(5,10))\n",
    "print(rdd1.collect())\n",
    "print(rdd2.collect())\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 11), (1, 12), (2, 11), (2, 12), (1, 13)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.cartesian(rdd2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 5, 6, 7]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.intersection(rdd3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checkpoint\n",
    "* Checkpoint current data of RDD\n",
    "* Need to first set dir, where data will be persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('ckpt')\n",
    "rdd.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coalesce\n",
    "* Return a new rdd with reduced partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cogroup\n",
    "* For each key k in self or other, return a resulting RDD that contains a tuple with the list of values for that key in self as well as other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4),(\"a\",9)])\n",
    "y = sc.parallelize([(\"a\",5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f87e910ee50>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f87e909fa10>)),\n",
       " ('b',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f87e909f0d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f87e909f210>))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd = x.cogroup(y)\n",
    "\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key a: \n",
      "[1, 9]\n",
      "[5]\n",
      "Key b: \n",
      "[4]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for e in new_rdd.collect():\n",
    "    print('Key {}: '.format(e[0]))\n",
    "    for l in e[1]:\n",
    "        print(list(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ([1, 9], [5])), ('b', ([4], []))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(a, map(list,b)) for a, b in new_rdd.collect()]\n",
    "[(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### countByValue\n",
    "* Returns a dict with value & counter corresponding to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'c': 2, 'd': 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(['a','c','a','d','c']).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combineByKey\n",
    "\n",
    "Generic function to combine the elements for each key using a custom set of aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('c', 18), ('a', 41)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In MyStr\n",
    "# In MyStr\n",
    "# In MyConcat\n",
    "# In MyConcat\n",
    "# In MyStr\n",
    "# In MyStr\n",
    "# In MyConcat\n",
    "# In MyConcat\n",
    "# In myPartConcat\n",
    "\n",
    "#Invoked per partition first time a key appears, d is the corresponding value \n",
    "def mystr(d):\n",
    "    print('In MyStr')\n",
    "    return d\n",
    "\n",
    "# 2nd time & onwards for same key in same partition\n",
    "def myconcat(a,b):\n",
    "    print('In MyConcat')\n",
    "    return a + b\n",
    "\n",
    "#Works across partitions\n",
    "def mypartConcat(a,b):\n",
    "    print('In myPartConcat')\n",
    "    return a + b\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2),(\"a\",8),(\"c\",4), (\"a\", 12),(\"a\",18),(\"c\",14)],2)\n",
    "\n",
    "#mystr - this converts the V into of type C\n",
    "rdd.combineByKey(mystr, myconcat, mypartConcat).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2]), ('b', [1])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 2:\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "\n",
    "#turns a V to C( create one-element list)\n",
    "def to_list(a):\n",
    "    return [a]\n",
    "\n",
    "# merges V into a C(adds it to the end of a list)\n",
    "def append(a,b):\n",
    "    a.append(b)\n",
    "\n",
    "# combines tow C's into a single one( merges the list)\n",
    "def extend(a,b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "x.combineByKey(to_list, append, extend).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1.0), ('c', 9.0), ('a', 8.2)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Invoked per partition first time a key appears, d is the corresponding value \n",
    "def mystr(d):\n",
    "    print('In MyStr')\n",
    "    return (d,1)\n",
    "\n",
    "# 2nd time & onwards for same key in same partition\n",
    "def myconcat(a,b):\n",
    "    print('In MyConcat')\n",
    "    return (a[0] + b,a[1]+1)\n",
    "\n",
    "#Works across partitions\n",
    "def mypartConcat(a,b):\n",
    "    print('In myPartConcat')\n",
    "    return (a[0] + b[0],a[1] + b[1])\n",
    "\n",
    "#mystr - this converts the V into of type C\n",
    "data = rdd.combineByKey(mystr, myconcat, mypartConcat)\n",
    "data.map(lambda x: (x[0], x[1][0]/(x[1][1]*1.0))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### countByKey\n",
    "* Count the number of elements for each key, and return the result to the master as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distinct\n",
    "* Return a new RDD containing the distinct elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 1, 2, 3]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter\n",
    "* selecting data conditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 10, 14, 18, 22, 26, 30, 34, 38]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(20))\n",
    "rdd.filter(lambda x: x%2 != 0).map(lambda x: x*2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flatMap\n",
    "* Convert 1 data to N data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 6, 7, 8, 7, 8, 9, 8, 9, 10]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([5,6,7,8])\n",
    "rdd.flatMap(lambda d: (d,d+1,d+2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first\n",
    "* returns first element of rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 6, 7, 8, 7, 8, 9, 8, 9, 10]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect\n",
    "* Gets data from all executor nodes to driver\n",
    "* Should be avoid if data is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4],2)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collectAsMap\n",
    "* Works only on PairRDD\n",
    "* Gets data to driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 6, 3: 4}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = sc.parallelize([(1, 2), (3, 4),(1,6)]).collectAsMap()\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foreach\n",
    "* Applies a function to all elements of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(e):\n",
    "    print(e)\n",
    "    \n",
    "#This print happens in each executor & not on driver\n",
    "#Chk on console if running on linux/aws\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foreachPartition\n",
    "* Applies function for each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(iterator):\n",
    "    for x in iterator:\n",
    "        print(x,)\n",
    "    print ('\\n Next Partition')\n",
    "        \n",
    "sc.parallelize([11, 12, 13, 14, 15],2).foreachPartition(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getNumPartitions\n",
    "* Returns number of partitions data is broken down into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([11, 12, 13, 14, 15],2)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StorageLevel\n",
    "\n",
    "* MEMORY_ONLY\tStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.\n",
    "* MEMORY_AND_DISK\tStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.\n",
    "* DISK_ONLY\tStore the RDD partitions only on disk.\n",
    "* MEMORY_ONLY_2, MEMORY_AND_DISK_2\tSame as the levels above, but replicate each partition on two cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getStorageLevel\n",
    "* return rdd storage location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "rdd1 = sc.parallelize([1,2])\n",
    "rdd1.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "print(rdd1.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomSplit\n",
    "* Split rdd elements into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(500), 1)\n",
    "rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce\n",
    "* Take two data & return one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduceByKey\n",
    "* Merge the values for each key using an associative and commutative reduce function.\n",
    "* This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repartition\n",
    "* Return a new RDD that has exactly numPartitions partitions.\n",
    "* Can increase or decrease the level of parallelism in this RDD. \n",
    "* Internally, this uses a shuffle to redistribute data. \n",
    "* If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5], [6, 7]]\n",
      "[[1, 4, 5, 6, 7], [2, 3]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.repartition(2).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [1], [4, 5, 6, 7], [2, 3], [], [], [], [], [], []]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(10).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saveAsTextFile\n",
    "* save rdd into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(range(10)).saveAsTextFile('data/abc.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortBy\n",
    "* Sorts this RDD by the given keyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortByKey\n",
    "* Sort based on keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5)],\n",
       " [('little', 4), ('Mary', 1), ('was', 8)],\n",
       " [('white', 9), ('whose', 6)]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(tmp2).sortByKey(ascending=True, numPartitions=3, keyfunc=lambda k: k.lower()).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subtract\n",
    "* subtracts only when key value pair is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 5), ('b', 4), ('a', 1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "x.subtract(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subtractByKey\n",
    "* Subtracts from the first rdd all the elements with the key present in second one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('b', 5)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "x.subtractByKey(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take\n",
    "* return first n elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### takeordered\n",
    "* Get the N elements from an RDD ordered in ascending order or as specified by the optional key function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 7, 6, 5, 4]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toDebugString\n",
    "* A description of this RDD and its recursive dependencies for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(4) ParallelCollectionRDD[126] at parallelize at PythonRDD.scala:195 []'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### values\n",
    "* Return value of each key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sc.parallelize([(1, 2), (3, 4)]).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zip\n",
    "* zips one rdd with another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipWithIndex\n",
    "* zip rdd element with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('b', 1), ('c', 2), ('d', 3)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
